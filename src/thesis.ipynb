{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing Packages\n",
    "Installing the required packages for the project.\n",
    "- `datasets`: A library for easily accessing and sharing datasets.\n",
    "- `openai`: The official Python library for the OpenAI API.\n",
    "- `python-dotenv`: A Python library for loading environment variables from a .env file.\n",
    "- `tqdm`: A fast, extensible progress bar for Python and CLI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /home/matin/miniconda3/lib/python3.12/site-packages (3.2.0)\n",
      "Requirement already satisfied: filelock in /home/matin/miniconda3/lib/python3.12/site-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/matin/miniconda3/lib/python3.12/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/matin/miniconda3/lib/python3.12/site-packages (from datasets) (19.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/matin/miniconda3/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/matin/miniconda3/lib/python3.12/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/matin/miniconda3/lib/python3.12/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /home/matin/miniconda3/lib/python3.12/site-packages (from datasets) (4.66.5)\n",
      "Requirement already satisfied: xxhash in /home/matin/miniconda3/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/matin/miniconda3/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /home/matin/miniconda3/lib/python3.12/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in /home/matin/miniconda3/lib/python3.12/site-packages (from datasets) (3.11.8)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in /home/matin/miniconda3/lib/python3.12/site-packages (from datasets) (0.27.1)\n",
      "Requirement already satisfied: packaging in /home/matin/miniconda3/lib/python3.12/site-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/matin/miniconda3/lib/python3.12/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/matin/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/matin/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/matin/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/matin/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/matin/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/matin/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/matin/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/matin/miniconda3/lib/python3.12/site-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/matin/miniconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/matin/miniconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/matin/miniconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/matin/miniconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/matin/miniconda3/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/matin/miniconda3/lib/python3.12/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/matin/miniconda3/lib/python3.12/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/matin/miniconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: openai in /home/matin/miniconda3/lib/python3.12/site-packages (1.60.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /home/matin/miniconda3/lib/python3.12/site-packages (from openai) (4.8.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /home/matin/miniconda3/lib/python3.12/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/matin/miniconda3/lib/python3.12/site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /home/matin/miniconda3/lib/python3.12/site-packages (from openai) (0.8.2)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /home/matin/miniconda3/lib/python3.12/site-packages (from openai) (2.9.2)\n",
      "Requirement already satisfied: sniffio in /home/matin/miniconda3/lib/python3.12/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /home/matin/miniconda3/lib/python3.12/site-packages (from openai) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /home/matin/miniconda3/lib/python3.12/site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in /home/matin/miniconda3/lib/python3.12/site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: certifi in /home/matin/miniconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /home/matin/miniconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/matin/miniconda3/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/matin/miniconda3/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /home/matin/miniconda3/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (2.23.4)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: python-dotenv in /home/matin/miniconda3/lib/python3.12/site-packages (1.0.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: tqdm in /home/matin/miniconda3/lib/python3.12/site-packages (4.66.5)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install datasets\n",
    "%pip install openai  \n",
    "%pip install python-dotenv\n",
    "%pip install tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports\n",
    "Importing the necessary modules and libraries for the project.\n",
    "- `load_dataset`: A function from the `datasets` library for loading datasets.\n",
    "- `os`: A module for interacting with the operating system.\n",
    "- `openai`: The official Python library for the OpenAI API.\n",
    "- `json`: A module for working with JSON data.\n",
    "- `random`: A module for generating random numbers.\n",
    "- `OpenAI`: A class from the `openai` library for interacting with the OpenAI API.\n",
    "- `load_dotenv`: A function from the `dotenv` library for loading environment variables from a .env file.\n",
    "- `tqdm`: A module for creating progress bars.\n",
    "- `Pool`: A class from the `multiprocessing` library for creating a pool of worker processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import os\n",
    "import openai\n",
    "import json\n",
    "import random\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the Dataset\n",
    "Loading the BC5CDR dataset using the `load_dataset` function from the `datasets` library. The dataset is a collection of biomedical texts annotated with chemical and disease mentions. The function returns a `DatasetDict` object containing the training, validation, and test splits of the dataset.\n",
    "\n",
    "Source: https://huggingface.co/datasets/tner/bc5cdr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['tokens', 'tags'],\n",
      "        num_rows: 5228\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['tokens', 'tags'],\n",
      "        num_rows: 5330\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['tokens', 'tags'],\n",
      "        num_rows: 5865\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nFirst three items.:\\n{'tokens': ['Naloxone', 'reverses', 'the', 'antihypertensive', 'effect', 'of', 'clonidine', '.'], 'tags': [1, 0, 0, 0, 0, 0, 1, 0]}\\n{'tokens': ['In', 'unanesthetized', ',', 'spontaneously', 'hypertensive', 'rats', 'the', 'decrease', 'in', 'blood', 'pressure', 'and', 'heart', 'rate', 'produced', 'by', 'intravenous', 'clonidine', ',', '5', 'to', '20', 'micrograms', '/', 'kg', ',', 'was', 'inhibited', 'or', 'reversed', 'by', 'nalozone', ',', '0', '.'], 'tags': [0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]}\\n{'tokens': ['2', 'to', '2', 'mg', '/', 'kg', '.'], 'tags': [0, 0, 0, 0, 0, 0, 0]}\\n\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "ds = load_dataset(\"tner/bc5cdr\")\n",
    "\n",
    "print(ds)\n",
    "# save dataset to json \n",
    "with open(\"bc5cdr_full.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(\n",
    "        {\n",
    "            \"train\": [item for item in ds[\"train\"]],\n",
    "        },\n",
    "        f,\n",
    "        ensure_ascii=False,\n",
    "        indent=2\n",
    "    )\n",
    "\"\"\"\n",
    "First three items.:\n",
    "{'tokens': ['Naloxone', 'reverses', 'the', 'antihypertensive', 'effect', 'of', 'clonidine', '.'], 'tags': [1, 0, 0, 0, 0, 0, 1, 0]}\n",
    "{'tokens': ['In', 'unanesthetized', ',', 'spontaneously', 'hypertensive', 'rats', 'the', 'decrease', 'in', 'blood', 'pressure', 'and', 'heart', 'rate', 'produced', 'by', 'intravenous', 'clonidine', ',', '5', 'to', '20', 'micrograms', '/', 'kg', ',', 'was', 'inhibited', 'or', 'reversed', 'by', 'nalozone', ',', '0', '.'], 'tags': [0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]}\n",
    "{'tokens': ['2', 'to', '2', 'mg', '/', 'kg', '.'], 'tags': [0, 0, 0, 0, 0, 0, 0]}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': ['Naloxone', 'reverses', 'the', 'antihypertensive', 'effect', 'of', 'clonidine', '.'], 'tags': [1, 0, 0, 0, 0, 0, 1, 0]}\n",
      "{'tokens': ['In', 'unanesthetized', ',', 'spontaneously', 'hypertensive', 'rats', 'the', 'decrease', 'in', 'blood', 'pressure', 'and', 'heart', 'rate', 'produced', 'by', 'intravenous', 'clonidine', ',', '5', 'to', '20', 'micrograms', '/', 'kg', ',', 'was', 'inhibited', 'or', 'reversed', 'by', 'nalozone', ',', '0', '.'], 'tags': [0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]}\n",
      "{'tokens': ['2', 'to', '2', 'mg', '/', 'kg', '.'], 'tags': [0, 0, 0, 0, 0, 0, 0]}\n",
      "5228\n",
      "5865\n",
      "5330\n"
     ]
    }
   ],
   "source": [
    "# print first 3 examples\n",
    "for i in range(3):\n",
    "    print(ds[\"train\"][i])\n",
    "\n",
    "\n",
    "# print length of ds\n",
    "print(len(ds[\"train\"]))\n",
    "print(len(ds[\"test\"]))\n",
    "print(len(ds[\"validation\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering Data\n",
    "Filtering data such that 'tags' include both 1 and 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter data such that 'tags' include both 1 and 2.\n",
    "filtered_ds = {}\n",
    "filtered_ds[\"train\"] = [example for example in ds[\"train\"] if 1 in example[\"tags\"] and 2 in example[\"tags\"]]\n",
    "filtered_ds[\"test\"] = [example for example in ds[\"test\"] if 1 in example[\"tags\"] and 2 in example[\"tags\"]]\n",
    "filtered_ds[\"validation\"] = [example for example in ds[\"validation\"] if 1 in example[\"tags\"] and 2 in example[\"tags\"]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 4, 0, 1, 4, 0, 0, 2, 0, 0, 0, 0, 0]\n",
      "[0, 0, 1, 0, 0, 0, 2, 3, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 0, 0, 0, 0, 0, 2, 3, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 4, 0, 2, 3, 3, 0, 0, 1, 0, 1, 0, 2, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 0, 0, 2, 3, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0]\n",
      "[0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 2, 0]\n",
      "[1, 0, 0, 2, 0, 2, 3, 0]\n",
      "[0, 2, 0, 1, 0]\n",
      "[0, 0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 2, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 2, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3]\n",
      "[0, 0, 0, 0, 0, 2, 3, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "1756\n",
      "1908\n",
      "{'tokens': ['Because', 'of', 'the', 'need', 'for', 'the', 'development', 'of', 'new', 'treatments', 'for', \"Crohn's\", 'disease', ',', 'a', 'pilot', 'study', 'was', 'undertaken', 'to', 'estimate', 'the', 'pharmacodynamics', 'and', 'tolerability', 'of', 'fusidic', 'acid', 'treatment', 'in', 'chronic', 'active', ',', 'therapy', '-', 'resistant', 'patients', '.'], 'tags': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "# print 3 random example's 'tags'.\n",
    "for i in range(15):\n",
    "    print(filtered_ds[\"train\"][random.randint(0, len(filtered_ds[\"train\"]))][\"tags\"])\n",
    "\n",
    "# print length of filtered_ds\n",
    "print(len(filtered_ds[\"train\"]))\n",
    "print(len(filtered_ds[\"test\"]))\n",
    "\n",
    "\n",
    "print(filtered_ds[\"train\"][15])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Because', 'of', 'the', 'need', 'for', 'the', 'development', 'of', 'new', 'treatments', 'for', \"Crohn's\", 'disease', ',', 'a', 'pilot', 'study', 'was', 'undertaken', 'to', 'estimate', 'the', 'pharmacodynamics', 'and', 'tolerability', 'of', 'fusidic', 'acid', 'treatment', 'in', 'chronic', 'active', ',', 'therapy', '-', 'resistant', 'patients', '.']\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "data = filtered_ds[\"train\"][15]\n",
    "tokens = data[\"tokens\"]\n",
    "tags = data[\"tags\"]\n",
    "\n",
    "print(tokens)\n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translating Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def translate_text(text, target_language):\n",
    "    client = OpenAI()\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are an expert in translating medicine-related scientific literature. Only provide the translated sentence.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Translate to {target_language}: {text}\"\n",
    "            }\n",
    "        ],\n",
    "        temperature=0.2,\n",
    "        max_tokens=2048,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    return text.split(\" \")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'token_string' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m translate_text(\u001b[43mtoken_string\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTurkish\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'token_string' is not defined"
     ]
    }
   ],
   "source": [
    "translate_text(token_string, \"Turkish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'token_string' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tokenized_translation \u001b[38;5;241m=\u001b[39m tokenize_text(translate_text(\u001b[43mtoken_string\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTurkish\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(tokenized_translation)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'token_string' is not defined"
     ]
    }
   ],
   "source": [
    "tokenized_translation = tokenize_text(translate_text(token_string, \"Turkish\"))\n",
    "print(tokenized_translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read tag_translator_system_prompt.txt and assign it to system_prompt variable\n",
    "system_prompt = open(\"tag_translator_system_prompt.txt\", \"r\").read()\n",
    "\n",
    "def translate_tags(system_prompt, original_tokens, original_tags, tokenized_translation):  \n",
    "    client = OpenAI()\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": [\n",
    "            {\n",
    "            \"type\": \"text\",\n",
    "            \"text\": system_prompt}\n",
    "        ]\n",
    "        },\n",
    "        {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "            \"type\": \"text\",\n",
    "            \"text\": f\"\"\"Based on information provided to you evaluate corresponding tags of words in the new translated set of words.\\nOriginal Tokens: {str(original_tokens)}\\n\\nOriginal Tags: {str(original_tags)}\\n\\nTranslated Tokens: {str(tokenized_translation)}\\n\\n\\nGive translated tags as a list of integers given in a json object.\\n\\n\n",
    "            json content => translated_tags: [integers separated by a comma]\"\"\"\n",
    "            }\n",
    "        ]\n",
    "        }\n",
    "    ],\n",
    "    response_format={\n",
    "        \"type\": \"json_object\"\n",
    "    },\n",
    "    temperature=0.2,\n",
    "    max_tokens=2048,\n",
    "    top_p=1,\n",
    "    frequency_penalty=0,\n",
    "    presence_penalty=0\n",
    "    )\n",
    "\n",
    "    # Extract the translated tags from the response\n",
    "    translated_tags = json.loads(response.choices[0].message.content)[\"translated_tags\"]\n",
    "    return translated_tags\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_tokens(element, target_language):\n",
    "    tokens = element[\"tokens\"]\n",
    "    token_string = \" \".join(tokens)\n",
    "    translated_string = translate_text(token_string, target_language)\n",
    "    translated_tokens = tokenize_text(translated_string)\n",
    "    return translated_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_element(element, target_language):\n",
    "    translated_tokens = translate_tokens(element, target_language)\n",
    "    translated_tags = translate_tags(system_prompt, element[\"tokens\"], element[\"tags\"], translated_tokens)\n",
    "    original_hash = hash(tuple(element[\"tokens\"]))\n",
    "    return {\"hash\": original_hash,\"original_tokens\": element[\"tokens\"], \"tokens\": translated_tokens, \"original_tags\": element[\"tags\"], \"tags\": translated_tags}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': ['Electrocardiographic', 'evidence', 'of', 'myocardial', 'injury', 'in', 'psychiatrically', 'hospitalized', 'cocaine', 'abusers', '.'], 'tags': [0, 0, 0, 2, 3, 0, 0, 0, 1, 0, 0]}\n",
      "{'hash': 3656629275778296211, 'original_tokens': ['Electrocardiographic', 'evidence', 'of', 'myocardial', 'injury', 'in', 'psychiatrically', 'hospitalized', 'cocaine', 'abusers', '.'], 'tokens': ['Psixiatrik', 'xəstəxanada', 'müalicə', 'olunan', 'kokain', 'istifadəçilərində', 'miyokard', 'zədələnməsinin', 'elektrokarioqrafik', 'sübutları.'], 'original_tags': [0, 0, 0, 2, 3, 0, 0, 0, 1, 0, 0], 'tags': [0, 0, 0, 2, 3, 0, 1, 0, 4, 0]}\n"
     ]
    }
   ],
   "source": [
    "random_element = filtered_ds[\"train\"][18]\n",
    "\n",
    "print(random_element)\n",
    "\n",
    "translated_element = translate_element(random_element, \"Azerbaijani\")\n",
    "print(translated_element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST: make sure filtered_ds now contains only first 30 elements of each split\n",
    "# filtered_ds = {split: data[:30] for split, data in filtered_ds.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing split 'train'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Core 10 processing split 'train':  53%|█████▎    | 62/117 [04:03<02:44,  2.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing element: Expecting ',' delimiter: line 2 column 2066 (char 2067)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Core 9 processing split 'train': 100%|██████████| 117/117 [06:54<00:00,  3.54s/it]]\n",
      "Core 4 processing split 'train': 100%|██████████| 117/117 [07:06<00:00,  3.64s/it]]\n",
      "Core 13 processing split 'train': 100%|██████████| 117/117 [07:09<00:00,  3.67s/it]\n",
      "Core 3 processing split 'train': 100%|██████████| 117/117 [07:11<00:00,  3.69s/it]]\n",
      "Core 5 processing split 'train': 100%|██████████| 117/117 [07:27<00:00,  3.82s/it]]\n",
      "Core 7 processing split 'train': 100%|██████████| 117/117 [07:27<00:00,  3.82s/it]\n",
      "Core 8 processing split 'train': 100%|██████████| 117/117 [07:27<00:00,  3.83s/it]\n",
      "Core 12 processing split 'train': 100%|██████████| 117/117 [07:27<00:00,  3.83s/it]\n",
      "Core 10 processing split 'train': 100%|██████████| 117/117 [07:31<00:00,  3.86s/it]\n",
      "Core 2 processing split 'train': 100%|██████████| 117/117 [07:32<00:00,  3.87s/it]\n",
      "Core 14 processing split 'train': 100%|██████████| 118/118 [07:48<00:00,  3.97s/it]\n",
      "Core 1 processing split 'train': 100%|██████████| 117/117 [07:59<00:00,  4.10s/it]]\n",
      "Core 6 processing split 'train': 100%|██████████| 117/117 [07:59<00:00,  4.10s/it]\n",
      "Core 11 processing split 'train': 100%|██████████| 117/117 [08:14<00:00,  4.23s/it]\n",
      "Core 0 processing split 'train': 100%|██████████| 117/117 [08:19<00:00,  4.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing split 'test'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Core 11 processing split 'test': 100%|██████████| 127/127 [07:17<00:00,  3.44s/it]\n",
      "Core 1 processing split 'test': 100%|██████████| 127/127 [07:18<00:00,  3.45s/it]]\n",
      "Core 0 processing split 'test': 100%|██████████| 127/127 [07:22<00:00,  3.48s/it]]\n",
      "Core 4 processing split 'test': 100%|██████████| 127/127 [07:27<00:00,  3.52s/it]]\n",
      "Core 6 processing split 'test': 100%|██████████| 127/127 [07:27<00:00,  3.53s/it]\n",
      "Core 5 processing split 'test': 100%|██████████| 127/127 [07:32<00:00,  3.56s/it]]\n",
      "Core 2 processing split 'test': 100%|██████████| 127/127 [07:33<00:00,  3.57s/it]\n",
      "Core 7 processing split 'test': 100%|██████████| 127/127 [07:34<00:00,  3.58s/it]]\n",
      "Core 3 processing split 'test': 100%|██████████| 127/127 [07:44<00:00,  3.66s/it]]\n",
      "Core 10 processing split 'test': 100%|██████████| 127/127 [07:45<00:00,  3.67s/it]\n",
      "Core 12 processing split 'test': 100%|██████████| 127/127 [07:48<00:00,  3.69s/it]\n",
      "Core 13 processing split 'test': 100%|██████████| 127/127 [07:56<00:00,  3.75s/it]\n",
      "Core 9 processing split 'test': 100%|██████████| 127/127 [08:02<00:00,  3.80s/it]]\n",
      "Core 14 processing split 'test': 100%|██████████| 130/130 [08:07<00:00,  3.75s/it]\n",
      "Core 8 processing split 'test': 100%|██████████| 127/127 [08:16<00:00,  3.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing split 'validation'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Core 14 processing split 'validation':  75%|███████▌  | 95/126 [05:08<01:40,  3.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing element: Expecting ',' delimiter: line 2 column 2064 (char 2065)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Core 14 processing split 'validation': 100%|██████████| 126/126 [06:47<00:00,  3.24s/it]\n",
      "Core 8 processing split 'validation': 100%|██████████| 125/125 [07:11<00:00,  3.45s/it]]\n",
      "Core 9 processing split 'validation': 100%|██████████| 125/125 [07:20<00:00,  3.53s/it]]\n",
      "Core 10 processing split 'validation': 100%|██████████| 125/125 [07:21<00:00,  3.53s/it]\n",
      "Core 11 processing split 'validation': 100%|██████████| 125/125 [07:23<00:00,  3.55s/it]\n",
      "Core 3 processing split 'validation': 100%|██████████| 125/125 [07:33<00:00,  3.63s/it]]\n",
      "Core 4 processing split 'validation': 100%|██████████| 125/125 [07:52<00:00,  3.78s/it]]\n",
      "Core 2 processing split 'validation': 100%|██████████| 125/125 [07:54<00:00,  3.80s/it]]\n",
      "Core 5 processing split 'validation': 100%|██████████| 125/125 [08:04<00:00,  3.88s/it]]\n",
      "Core 1 processing split 'validation': 100%|██████████| 125/125 [08:08<00:00,  3.91s/it]]\n",
      "Core 7 processing split 'validation': 100%|██████████| 125/125 [08:15<00:00,  3.97s/it]]\n",
      "Core 13 processing split 'validation': 100%|██████████| 125/125 [08:29<00:00,  4.08s/it]\n",
      "Core 0 processing split 'validation': 100%|██████████| 125/125 [08:42<00:00,  4.18s/it]]\n",
      "Core 12 processing split 'validation': 100%|██████████| 125/125 [08:53<00:00,  4.27s/it]\n",
      "Core 6 processing split 'validation': 100%|██████████| 125/125 [08:54<00:00,  4.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final dataset saved to translated_ds_tr.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Define the translation function for parallel processing\n",
    "def process_element(args):\n",
    "    element, language = args\n",
    "    try:\n",
    "        return translate_element(element, language)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing element: {e}\")\n",
    "        return None\n",
    "\n",
    "def save_progress(filename, data):\n",
    "    try:\n",
    "        with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(data, f, ensure_ascii=False)\n",
    "    except IOError as e:\n",
    "        print(f\"Error saving progress: {e}\")\n",
    "\n",
    "def worker_function(args):\n",
    "    core_index, chunk, split_name = args\n",
    "    translated_chunk = []\n",
    "    for element in tqdm(chunk, desc=f\"Core {core_index} processing split '{split_name}'\"):\n",
    "        translated = process_element((element, \"Turkish\"))\n",
    "        if translated is not None:\n",
    "            translated_chunk.append(translated)\n",
    "    return translated_chunk\n",
    "\n",
    "def translate_split(split_name, split_data, core_count):\n",
    "    chunk_size = len(split_data) // core_count\n",
    "    \n",
    "    progress_files = []\n",
    "    tasks = []\n",
    "    for core_index in range(core_count):\n",
    "        start_index = core_index * chunk_size\n",
    "        end_index = start_index + chunk_size if core_index < core_count - 1 else len(split_data)\n",
    "        core_chunk = split_data[start_index:end_index]\n",
    "\n",
    "        progress_file = f\"progress_core_{core_index}_{split_name}.json\"\n",
    "        progress_files.append(progress_file)\n",
    "        tasks.append((core_index, core_chunk, split_name))\n",
    "\n",
    "    # Process tasks in parallel\n",
    "    with Pool(core_count) as pool:\n",
    "        results = pool.map(worker_function, tasks)\n",
    "\n",
    "    # Save progress for each core\n",
    "    for core_index, result in enumerate(results):\n",
    "        if result:\n",
    "            save_progress(progress_files[core_index], result)\n",
    "\n",
    "    # Combine all core results for the split\n",
    "    translated_split = []\n",
    "    for file in progress_files:\n",
    "        if os.path.exists(file):\n",
    "            try:\n",
    "                with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "                    translated_split.extend(json.load(f))\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading progress file {file}: {e}\")\n",
    "\n",
    "    return translated_split\n",
    "\n",
    "# Main translation logic\n",
    "translated_ds = {}\n",
    "output_file = \"translated_ds_tr.json\"\n",
    "core_count = 15\n",
    "\n",
    "for split_name, split_data in filtered_ds.items():\n",
    "    print(f\"Processing split '{split_name}'...\")\n",
    "    translated_ds[split_name] = translate_split(split_name, split_data, core_count)\n",
    "\n",
    "# Save the final translated dataset\n",
    "try:\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(translated_ds, f, ensure_ascii=False)\n",
    "    print(f\"Final dataset saved to {output_file}\")\n",
    "\n",
    "    # Clean up progress files\n",
    "    for split_name in filtered_ds.keys():\n",
    "        for core_index in range(core_count):\n",
    "            progress_file = f\"progress_core_{core_index}_{split_name}.json\"\n",
    "            if os.path.exists(progress_file):\n",
    "                os.remove(progress_file)\n",
    "except IOError as e:\n",
    "    print(f\"Error saving the final dataset: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
